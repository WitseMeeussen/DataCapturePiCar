{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python standard libraries\n",
    "import os\n",
    "import random\n",
    "import fnmatch\n",
    "import datetime\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# data processing\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float_kind':lambda x: \"%.4f\" % x})\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 300)\n",
    "pd.set_option('display.float_format', '{:,.4f}'.format)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential  # V2 is tensorflow.keras.xxxx, V1 is keras.xxx\n",
    "from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense\n",
    "from keras.models import load_model\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# imaging\n",
    "import cv2\n",
    "from imgaug import augmenters as img_aug\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapDirection(dir):\n",
    "    if(int(dir) == -1):\n",
    "        return 0\n",
    "    if(int(dir) == 1):\n",
    "        return 90\n",
    "    if(int(dir) == 0):\n",
    "        return 180\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './Data'\n",
    "data_folders = os.listdir(data_dir)\n",
    "file_list = []\n",
    "\n",
    "for dir in data_folders:\n",
    "    file_list = file_list + list(map(lambda d: dir+ \"/\" + d,os.listdir(os.path.join(data_dir,dir))))\n",
    "image_paths = []\n",
    "steering_direction = []\n",
    "pattern = \"*.png\"\n",
    "for filename in file_list:\n",
    "    if fnmatch.fnmatch(filename, pattern):\n",
    "        image_paths.append(os.path.join(data_dir,filename))\n",
    "        direction = re.sub('.png','',re.sub('.*dir_','',filename))  # 092 part of video01_143_092.png is the angle. 90 is go straight\n",
    "        steering_direction.append(mapDirection(direction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the validation set is not a good split. But we want to train the model on as much data as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 435\n",
      "Validation data: 49\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split( image_paths, steering_direction, test_size=0.1)\n",
    "print(\"Training data: %d\\nValidation data: %d\" % (len(X_train), len(X_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom(image):\n",
    "    zoom = img_aug.Affine(scale=(1.1, 1.3))  # zoom from 100% (no zoom) to 130%\n",
    "    image = zoom.augment_image(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random augmentation of the images provides a more robust data set.\n",
    "Because of a miss alignment of the wheels of the car we can not flip these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_augment(image, steering_angle):\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = zoom(image)\n",
    "    return image, steering_angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image processing is necesary to use the images with the nvidia model.\n",
    "\n",
    "Documentation about the nvidia model says that it is better to convert the image from RGB to YUV.\n",
    "This line is comented because of an issue when converting live images to YUV while the car is autonomous driving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_preprocess(image):\n",
    "    height, _, _ = image.shape\n",
    "    image = image[int(height/2):,:,:]  # remove top half of the image, as it is not relevant for lane following\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)  # Nvidia model said it is best to use YUV color space\n",
    "    image = cv2.GaussianBlur(image, (3,3), 0)\n",
    "    image = cv2.resize(image, (200,66)) # input image size (200,66) Nvidia model\n",
    "    image = image / 255 # normalizing the values of the image. We dont need to normalize it in the model anny more\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research teached us that the nvidea model is an efficiÃ«nt model to use for autonomous driving.\n",
    "\n",
    "It uses elu activation functions, this is usefull for training the model (this is becaus unlike the relu it backpropagates negative values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Nvidia_Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 31, 98, 24)        1824      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 47, 36)        21636     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 5, 22, 48)         43248     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 20, 64)         27712     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 1, 18, 64)         36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               115300    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 252,219\n",
      "Trainable params: 252,219\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def nvidia_model():\n",
    "    model = Sequential(name='Nvidia_Model')\n",
    "    \n",
    "    # elu=Expenential Linear Unit, similar to leaky Relu\n",
    "    # skipping 1st hiddel layer (nomralization layer), as we have normalized the data\n",
    "    \n",
    "    # Convolution Layers\n",
    "    model.add(Conv2D(24, (5, 5), strides=(2, 2), input_shape=(66, 200, 3), activation='elu')) \n",
    "    model.add(Conv2D(36, (5, 5), strides=(2, 2), activation='elu')) \n",
    "    model.add(Conv2D(48, (5, 5), strides=(2, 2), activation='elu')) \n",
    "    model.add(Conv2D(64, (3, 3), activation='elu')) \n",
    "    model.add(Dropout(0.2)) # not in original model. added for more robustness\n",
    "    model.add(Conv2D(64, (3, 3), activation='elu')) \n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2)) # not in original model. added for more robustness\n",
    "    model.add(Dense(100, activation='elu'))\n",
    "    model.add(Dense(50, activation='elu'))\n",
    "    model.add(Dense(10, activation='elu'))\n",
    "    \n",
    "    # output layer: turn angle (from 45-135, 90 is straight, <90 turn left, >90 turn right)\n",
    "    model.add(Dense(activation=\"sigmoid\", units=1))\n",
    "    \n",
    "    # since this is a regression problem not classification problem\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_hinge', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = nvidia_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_data_generator(image_paths, steering_angles, batch_size, is_training):\n",
    "    while True:\n",
    "        batch_images = []\n",
    "        batch_steering_angles = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            random_index = random.randint(0, len(image_paths) - 1)\n",
    "            image_path = image_paths[random_index]\n",
    "            image = cv2.imread(image_paths[random_index])\n",
    "            steering_angle = steering_angles[random_index]\n",
    "            if is_training:\n",
    "                # training: augment image\n",
    "                image, steering_angle = random_augment(image, steering_angle)\n",
    "              \n",
    "            image = img_preprocess(image)\n",
    "            batch_images.append(image)\n",
    "            batch_steering_angles.append(steering_angle)\n",
    "            \n",
    "        yield( np.asarray(batch_images), np.asarray(batch_steering_angles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\32492\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/300 [..............................] - ETA: 7:47 - loss: 0.5064 - accuracy: 0.3275"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22376/3223627241.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcheckpoint_callback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'lane_navigation_check.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m history = model.fit_generator(image_data_generator( X_train, y_train, batch_size=100, is_training=True),\n\u001b[0m\u001b[0;32m      8\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1973\u001b[0m                   \u001b[1;34m'will be removed in a future version. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1974\u001b[0m                   'Please use `Model.fit`, which supports generators.')\n\u001b[1;32m-> 1975\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   1976\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1977\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_output_dir = \"./Model\"\n",
    "\n",
    "\n",
    "# saves the model weights after each epoch if the validation loss decreased\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(model_output_dir,'lane_navigation_check.h5'), verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit_generator(image_data_generator( X_train, y_train, batch_size=100, is_training=True),\n",
    "                              steps_per_epoch=300,\n",
    "                              epochs=1,\n",
    "                              validation_data = image_data_generator( X_valid, y_valid, batch_size=100, is_training=False),\n",
    "                              validation_steps=200,\n",
    "                              verbose=1,\n",
    "                              shuffle=1,\n",
    "                              callbacks=[checkpoint_callback])\n",
    "# always save model output as soon as model finishes training\n",
    "model.save(os.path.join(model_output_dir,'lane_navigation_final.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81.9020]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model('./Model/lane_navigation_check.h5')\n",
    "        \n",
    "def compute_steering_angle( frame):\n",
    "    preprocessed = img_preprocess(frame)\n",
    "    X = np.asarray([preprocessed])\n",
    "    steering_angle = model.predict(X)[0]\n",
    "    return steering_angle\n",
    "image = cv2.imread('./Data/capture3/nr3_15_dir_-1.png')\n",
    "print(compute_steering_angle(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\32492\\AppData\\Local\\Temp\\tmpnozcpe4r\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\32492\\AppData\\Local\\Temp\\tmpnozcpe4r\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(load_model('./Model/lane_navigation_check.h5'))\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7713304168cbd30576c08cd8037e755ec502527db886bcfa010c8949df6f0886"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
