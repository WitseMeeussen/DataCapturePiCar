{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python standard libraries\n",
    "import os\n",
    "import random\n",
    "import fnmatch\n",
    "import datetime\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# data processing\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float_kind':lambda x: \"%.4f\" % x})\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 300)\n",
    "pd.set_option('display.float_format', '{:,.4f}'.format)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential  # V2 is tensorflow.keras.xxxx, V1 is keras.xxx\n",
    "from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense\n",
    "from keras.models import load_model\n",
    "\n",
    "# sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# imaging\n",
    "import cv2\n",
    "from imgaug import augmenters as img_aug\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test0.png', 'test1.png', 'test10.png', 'test100.png', 'test101.png', 'test102.png', 'test103.png', 'test104.png', 'test105.png', 'test106.png', 'test107.png', 'test108.png', 'test109.png', 'test11.png', 'test110.png', 'test111.png', 'test112.png', 'test113.png', 'test114.png', 'test115.png', 'test116.png', 'test117.png', 'test118.png', 'test119.png', 'test12.png', 'test120.png', 'test121.png', 'test122.png', 'test123.png', 'test124.png', 'test125.png', 'test126.png', 'test127.png', 'test128.png', 'test129.png', 'test13.png', 'test130.png', 'test131.png', 'test132.png', 'test133.png', 'test134.png', 'test135.png', 'test136.png', 'test137.png', 'test138.png', 'test139.png', 'test14.png', 'test140.png', 'test141.png', 'test142.png', 'test143.png', 'test144.png', 'test145.png', 'test146.png', 'test147.png', 'test148.png', 'test149.png', 'test15.png', 'test150.png', 'test151.png', 'test152.png', 'test153.png', 'test154.png', 'test155.png', 'test156.png', 'test16.png', 'test17.png', 'test18.png', 'test19.png', 'test2.png', 'test20.png', 'test21.png', 'test22.png', 'test23.png', 'test24.png', 'test25.png', 'test26.png', 'test27.png', 'test28.png', 'test29.png', 'test3.png', 'test30.png', 'test31.png', 'test32.png', 'test33.png', 'test34.png', 'test35.png', 'test36.png', 'test37.png', 'test38.png', 'test39.png', 'test4.png', 'test40.png', 'test41.png', 'test42.png', 'test43.png', 'test44.png', 'test45.png', 'test46.png', 'test47.png', 'test48.png', 'test49.png', 'test5.png', 'test50.png', 'test51.png', 'test52.png', 'test53.png', 'test54.png', 'test55.png', 'test56.png', 'test57.png', 'test58.png', 'test59.png', 'test6.png', 'test60.png', 'test61.png', 'test62.png', 'test63.png', 'test64.png', 'test65.png', 'test66.png', 'test67.png', 'test68.png', 'test69.png', 'test7.png', 'test70.png', 'test71.png', 'test72.png', 'test73.png', 'test74.png', 'test75.png', 'test76.png', 'test77.png', 'test78.png', 'test79.png', 'test8.png', 'test80.png', 'test81.png', 'test82.png', 'test83.png', 'test84.png', 'test85.png', 'test86.png', 'test87.png', 'test88.png', 'test89.png', 'test9.png', 'test90.png', 'test91.png', 'test92.png', 'test93.png', 'test94.png', 'test95.png', 'test96.png', 'test97.png', 'test98.png', 'test99.png', '0_0.png', '10_1.png', '11_0.png', '12_0.png', '13_0.png', '14_0.png', '15_0.png', '16_0.png', '17_0.png', '18_0.png', '19_0.png', '1_0.png', '2_0.png', '3_0.png', '4_0.png', '5_0.png', '6_0.png', '7_0.png', '8_0.png', '9_0.png', '0_0.png', '10_0.png', '11_0.png', '12_0.png', '1_0.png', '2_1.png', '3_0.png', '4_0.png', '5_0.png', '6_-1.png', '7_1.png', '8_0.png', '9_0.png', '0_0.png', '100_0.png', '101_0.png', '102_0.png', '103_0.png', '104_0.png', '105_0.png', '106_0.png', '107_0.png', '108_0.png', '109_0.png', '10_0.png', '110_0.png', '11_0.png', '12_0.png', '13_0.png', '14_0.png', '15_0.png', '16_0.png', '17_0.png', '18_0.png', '19_0.png', '1_0.png', '20_0.png', '21_0.png', '22_0.png', '23_0.png', '24_0.png', '25_0.png', '26_0.png', '27_0.png', '28_0.png', '29_0.png', '2_0.png', '30_0.png', '31_0.png', '32_0.png', '33_0.png', '34_0.png', '35_0.png', '36_0.png', '37_0.png', '38_0.png', '39_0.png', '3_0.png', '40_0.png', '41_0.png', '42_0.png', '43_0.png', '44_0.png', '45_0.png', '46_0.png', '47_0.png', '48_0.png', '49_0.png', '4_0.png', '50_0.png', '51_0.png', '52_0.png', '53_0.png', '54_0.png', '55_0.png', '56_0.png', '57_0.png', '58_0.png', '59_0.png', '5_0.png', '60_0.png', '61_0.png', '62_0.png', '63_0.png', '64_0.png', '65_0.png', '66_0.png', '67_0.png', '68_0.png', '69_0.png', '6_0.png', '70_0.png', '71_0.png', '72_0.png', '73_0.png', '74_0.png', '75_0.png', '76_0.png', '77_0.png', '78_0.png', '79_0.png', '7_0.png', '80_0.png', '81_0.png', '82_0.png', '83_0.png', '84_0.png', '85_0.png', '86_0.png', '87_0.png', '88_0.png', '89_0.png', '8_0.png', '90_0.png', '91_0.png', '92_0.png', '93_0.png', '94_0.png', '95_0.png', '96_0.png', '97_0.png', '98_0.png', '99_0.png', '9_0.png', '0_0.png', '10_1.png', '11_1.png', '12_1.png', '13_1.png', '14_1.png', '15_1.png', '16_1.png', '17_0.png', '18_-1.png', '19_0.png', '1_0.png', '20_0.png', '21_1.png', '22_1.png', '23_1.png', '24_1.png', '25_1.png', '26_1.png', '27_1.png', '28_1.png', '29_1.png', '2_1.png', '30_1.png', '31_1.png', '32_1.png', '33_1.png', '34_1.png', '35_0.png', '36_-1.png', '37_-1.png', '38_-1.png', '39_-1.png', '3_1.png', '40_-1.png', '41_-1.png', '42_-1.png', '43_-1.png', '44_0.png', '45_1.png', '46_1.png', '47_-1.png', '48_-1.png', '49_-1.png', '4_0.png', '50_-1.png', '51_0.png', '52_0.png', '53_-1.png', '54_0.png', '55_0.png', '56_-1.png', '57_-1.png', '58_-1.png', '59_-1.png', '5_0.png', '60_-1.png', '61_-1.png', '62_-1.png', '63_0.png', '64_0.png', '65_1.png', '66_0.png', '67_0.png', '68_0.png', '69_1.png', '6_1.png', '70_1.png', '71_1.png', '72_0.png', '73_0.png', '74_0.png', '75_0.png', '76_1.png', '77_1.png', '78_1.png', '79_1.png', '7_1.png', '80_0.png', '81_0.png', '8_0.png', '9_0.png']\n"
     ]
    }
   ],
   "source": [
    "data_dir = './Data'\n",
    "data_folders = os.listdir(data_dir)\n",
    "file_list = []\n",
    "for dir in data_folders:\n",
    "    file_list = file_list + os.listdir(os.path.join(data_dir,dir))\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './Data'\n",
    "data_folders = os.listdir(data_dir)\n",
    "file_list = []\n",
    "for dir in data_folders:\n",
    "    file_list = file_list + os.listdir(os.path.join(data_dir,dir))\n",
    "    \n",
    "image_paths = []\n",
    "steering_direction = []\n",
    "pattern = \"*.png\"\n",
    "for filename in file_list:\n",
    "    if fnmatch.fnmatch(filename, pattern):\n",
    "        image_paths.append(os.path.join(data_dir,filename))\n",
    "        angle = re.sub('.png','',re.sub('.*dir_','',filename))  # 092 part of video01_143_092.png is the angle. 90 is go straight\n",
    "        steering_direction.append(angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the validation set is not a good split. But we want to train the model on as much data as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split( image_paths, steering_direction, test_size=0.1)\n",
    "print(\"Training data: %d\\nValidation data: %d\" % (len(X_train), len(X_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom(image):\n",
    "    zoom = img_aug.Affine(scale=(1.1, 1.3))  # zoom from 100% (no zoom) to 130%\n",
    "    image = zoom.augment_image(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip(image, steering_direction):\n",
    "    image = cv2.flip(image,1)\n",
    "    steering_direction = -steering_direction\n",
    "    return image, steering_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_augment(image, steering_angle):\n",
    "    # if np.random.rand() < 0.5:\n",
    "    #     image = zoom(image)\n",
    "    # if np.random.rand() < 0.5:\n",
    "    #     image = adjust_brightness(image)\n",
    "    # image, steering_angle = random_flip(image, steering_angle)\n",
    "    \n",
    "    return image, steering_angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image processing is necesary to use the images with the nvidia model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_preprocess(image):\n",
    "    height, _, _ = image.shape\n",
    "    image = image[int(height/2):,:,:]  # remove top half of the image, as it is not relevant for lane following\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)  # Nvidia model said it is best to use YUV color space\n",
    "    image = cv2.GaussianBlur(image, (3,3), 0)\n",
    "    image = cv2.resize(image, (200,66)) # input image size (200,66) Nvidia model\n",
    "    image = image / 255 # normalizing the values of the image. We dont need to normalize it in the model anny more\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown activation function: asdf. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22320/3173760054.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnvidia_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22320/3173760054.py\u001b[0m in \u001b[0;36mnvidia_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# output layer: turn angle (from 45-135, 90 is straight, <90 turn left, >90 turn right)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"asdf\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# since this is a regression problem not classification problem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m       raise ValueError(f'Received an invalid value for `units`, expected '\n\u001b[0;32m   1153\u001b[0m                        f'a positive integer, got {units}.')\n\u001b[1;32m-> 1154\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1155\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\activations.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    594\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[0midentifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\activations.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(name, custom_objects)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[0mglobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m   return deserialize_keras_object(\n\u001b[0m\u001b[0;32m    556\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    702\u001b[0m       \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule_objects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    705\u001b[0m             \u001b[1;34m'Unknown {}: {}. Please ensure this object is '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m             \u001b[1;34m'passed to the `custom_objects` argument. See '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown activation function: asdf. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "def nvidia_model():\n",
    "    model = Sequential(name='Nvidia_Model')\n",
    "    \n",
    "    # elu=Expenential Linear Unit, similar to leaky Relu\n",
    "    # skipping 1st hiddel layer (nomralization layer), as we have normalized the data\n",
    "    \n",
    "    # Convolution Layers\n",
    "    model.add(Conv2D(24, (5, 5), strides=(2, 2), input_shape=(66, 200, 3), activation='elu')) \n",
    "    model.add(Conv2D(36, (5, 5), strides=(2, 2), activation='elu')) \n",
    "    model.add(Conv2D(48, (5, 5), strides=(2, 2), activation='elu')) \n",
    "    model.add(Conv2D(64, (3, 3), activation='elu')) \n",
    "    model.add(Dropout(0.2)) # not in original model. added for more robustness\n",
    "    model.add(Conv2D(64, (3, 3), activation='elu')) \n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2)) # not in original model. added for more robustness\n",
    "    model.add(Dense(100, activation='elu'))\n",
    "    model.add(Dense(50, activation='elu'))\n",
    "    model.add(Dense(10, activation='elu'))\n",
    "    \n",
    "    # output layer: turn angle (from 45-135, 90 is straight, <90 turn left, >90 turn right)\n",
    "    model.add(Dense(activation=\"softmax\", units=3))\n",
    "    \n",
    "    # since this is a regression problem not classification problem\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_hinge', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = nvidia_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_data_generator(image_paths, steering_angles, batch_size, is_training):\n",
    "    while True:\n",
    "        batch_images = []\n",
    "        batch_steering_angles = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            random_index = random.randint(0, len(image_paths) - 1)\n",
    "            image_path = image_paths[random_index]\n",
    "            image = cv2.imread(image_paths[random_index])\n",
    "            steering_angle = steering_angles[random_index]\n",
    "            if is_training:\n",
    "                # training: augment image\n",
    "                image, steering_angle = random_augment(image, steering_angle)\n",
    "              \n",
    "            image = img_preprocess(image)\n",
    "            batch_images.append(image)\n",
    "            batch_steering_angles.append(steering_angle)\n",
    "            \n",
    "        yield( np.asarray(batch_images), np.asarray(batch_steering_angles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dir = \"./Model\"\n",
    "\n",
    "\n",
    "# saves the model weights after each epoch if the validation loss decreased\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(model_output_dir,'lane_navigation_check.h5'), verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit_generator(image_data_generator( X_train, y_train, batch_size=100, is_training=True),\n",
    "                              steps_per_epoch=300,\n",
    "                              epochs=10,\n",
    "                              validation_data = image_data_generator( X_valid, y_valid, batch_size=100, is_training=False),\n",
    "                              validation_steps=200,\n",
    "                              verbose=1,\n",
    "                              shuffle=1,\n",
    "                              callbacks=[checkpoint_callback])\n",
    "# always save model output as soon as model finishes training\n",
    "model.save(os.path.join(model_output_dir,'lane_navigation_final.h5'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7713304168cbd30576c08cd8037e755ec502527db886bcfa010c8949df6f0886"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
